import streamlit as st
import os

from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory

# ================== PAGE CONFIG ==================
st.set_page_config(
    page_title="AI Health Buddy ü©∫",
    page_icon="ü©∫",
    layout="centered"
)

st.title("ü©∫ AI Health Buddy")
st.caption("‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏Å‡∏£‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ (Groq ‚Ä¢ RAG ‚Ä¢ Chat)")

# ================== LOAD API KEY ==================
groq_api_key = st.secrets.get("GROQ_API_KEY", os.getenv("GROQ_API_KEY"))

if not groq_api_key:
    st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö GROQ_API_KEY ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡πÉ‡∏ô Streamlit Secrets")
    st.stop()

# ================== PROMPT ==================
prompt_template = """
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ "AI Health Buddy" ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏†‡∏™‡∏±‡∏ä‡∏Å‡∏£‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û ‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£ ‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢

‡∏Å‡∏ï‡∏¥‡∏Å‡∏≤:
- ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Context ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
- ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏î‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ
- ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏ö‡∏ö‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥
- ‡∏ï‡∏≠‡∏ö‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
- ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡∏≤‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏∏‡∏†‡∏≤‡∏û
- ‡∏´‡∏•‡∏±‡∏á‡∏ï‡∏≠‡∏ö ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏Å‡∏•‡∏±‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°

Context:
{context}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ:
{question}

‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:
"""

PROMPT = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)

# ================== LOAD SYSTEM ==================
@st.cache_resource
def load_system():

    # LLM
    llm = ChatGroq(
        groq_api_key=groq_api_key,
        model_name="llama-3.3-70b-versatile",
        temperature=0.2
    )

    # Embeddings
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    # ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå knowledge
    loader = TextLoader("data/clean_knowledge.txt", encoding="utf-8")
    documents = loader.load()

    # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=100
    )
    texts = text_splitter.split_documents(documents)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Vector DB ‡πÅ‡∏ö‡∏ö runtime
    db = Chroma.from_documents(
        texts,
        embeddings
    )

    retriever = db.as_retriever(search_kwargs={"k": 2})

    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True
    )

    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        combine_docs_chain_kwargs={"prompt": PROMPT}
    )

    return qa_chain


qa = load_system()

# ================== SESSION STATE ==================
if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "assistant",
            "content": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö üòä ‡∏ú‡∏°‡∏Ñ‡∏∑‡∏≠ AI Health Buddy\n‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ú‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÑ‡∏î‡πâ‡∏ö‡πâ‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡∏ö?"
        }
    ]

# ================== SHOW CHAT ==================
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ================== INPUT ==================
user_input = st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà...")

if user_input:
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° user
    st.session_state.messages.append(
        {"role": "user", "content": user_input}
    )
    with st.chat_message("user"):
        st.markdown(user_input)

    # ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö
    with st.chat_message("assistant"):
        with st.spinner("üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏Å‡∏≤‡∏£..."):
            try:
                result = qa.invoke({"question": user_input})
                ai_reply = result["answer"]
            except Exception as e:
                ai_reply = f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}"

            st.markdown(ai_reply)

    st.session_state.messages.append(
        {"role": "assistant", "content": ai_reply}
    )

st.divider()
st.caption("‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå")
